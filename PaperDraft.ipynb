{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribution to the 2020 BraTS Challenge: Automated Segmentation using 3D Attention UNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessment of brain tumors is important in the diagnostic of Cancer [Cite Medicine Paper]. Automatic segmentation can aid in this assessment, allowing for description of relevant tumor features such as its volume. However, tumors are very heterogeneous in shape, having different associated grades and classifications. Due to this variance, automatic segmentation of brain tumors is still a challenge [BraTS Journal].\n",
    "\n",
    "A source of public Glioma brain tumors is the BraTS challenge [BraTS Journal]. This challenge expects high quality automatic segmentations of glioma regions, annotated over the provided four modalities of magnetic ressonance imaging (MRI). The conception of the challenge came from the high inter-rater disagreement between expert raters in 2012 of up to 0.74 DICE [GDL]. Currently, most top-ranking methods in the challenge use Deep Learning [Deep Learning Nature] based methods. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![intro_fig](figures/tumor_brats.png)\n",
    "Figure 1: A sample manual annotation of a glioma. (a) edema (yellow), (b) non-enhancing solid core (red), (c) necrotic/cystic core (green), enhancing core(blue). (d) Combined segmentations. Reproduced from {menze2014multimodal}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptation of the famous UNet [Ronnenberg] architecture is a common approach in recent years, with many succesful methods [(Kamnitsas, 2017), (Isensee, 2017), (Myronenko, 2018), (Myronenko, 2019), (Jyang, 2019)]. \n",
    "\n",
    "Kamnitsas Et al. [Kamnitsas, 2017] achieved top perfomance using an ensemble of four medical image segmentation CNN architectures, including 2 U-Net based ones, winning BRATS in 2017. The author proposes that its ensemble strategy aims to reduce the impact of different hyperparameters and bias employed to each architecture, by averaging their results.\n",
    "\n",
    "Isensee Et al.'s [Isensee, 2017] method adapted the UNet for 3D convolutions, with more skip connections, less channels, intensive augmentation, and a multi-class adaptation of DICE Loss that would be later called Generalized Dice Loss [GDL]. Interestingly, Isensee Et al. is one of the leading methods from the 2017 challenge using mostly a single U-Net architecture, showing that a well trained U-Net can be superior to complex ensemble approaches.\n",
    "\n",
    "The winner of the 2018 challenge also used an U-Net like architecture [Myronenko, 2018]. The main novelty of this work consisted of using a second branch in the decoder part of the architecture, reconstructing the original image as a means of regularization of the encoder. Another difference to basic U-Net is the use of a larger encoder, while most works keep the symmetry between encoder and decoder. \n",
    "\n",
    "(Myronenko, 2019)...\n",
    "\n",
    "(Jyang, 2019)...\n",
    "\n",
    "UNet like architectures have been achieving top performance in BraTS year after year. This paper proposes to further explore the potential of the UNet encoder-decoder architecture, with the use of attention modules and other modifications recently featured in the literature..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executable Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an executable paper. The code cells can be run to reproduce important steps of this research. The following cell will setup the enviroment. \n",
    "\n",
    "MLflow is used to manage experiment runs, logging to the cell output what command was executed. Details from each run can be visualized by runnning \"mlflow ui\" on the directory of this notebook and accessing http://localhost:5000 on your browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE CELL: 0\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BraTS 2020 scans are available as NIfTI files of various modalities: T1, post-contrast T1, T2, and FLAIR volumes, acquired with different clinical protocols and various scanners from multiple institutions[BraTS].\n",
    "\n",
    "Subjects have manual segmentation, performed by one to four raters, following the same protocol, with the resulting segmentation being approved by experienced neuro-radiologists. Annotations comprise the GD-enhancing tumor, the peritumoral edema, and the necrotic and non-enhancing tumor core, as described in the latest BraTS summarizing paper [BratS]. The provided data are distributed after pre-processing: co-registration to the same anatomical template, interpolation to the same resolution and skull-stripping.\n",
    "\n",
    "Additional pre-processing applied in this paper follows Isensee Et al.'s [Isensee] pre-processing. The images are subtracted by the mean and divided by the standard deviation of the brain region, and clipped inside the interval -5 to 5. Finally, they are min-max normalized to the interval 0 to 1.\n",
    "\n",
    "This pre processing can be reproduced by code cell 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020/05/23 18:19:30 INFO mlflow.projects: === Created directory /tmp/tmpzby1a51p for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2020/05/23 18:19:30 INFO mlflow.projects: === Running command 'python3 src/brats_preprocess.py' in run with ID '8f2fdb6d11f742bc920a1f5a70061f89' === \n",
      "2020/05/23 18:26:53 INFO mlflow.projects: === Run (ID '8f2fdb6d11f742bc920a1f5a70061f89') succeeded ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.projects.submitted_run.LocalSubmittedRun at 0x7f41fa239f28>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CODE CELL: 1\n",
    "# TODO use someway to distribute data. Currently data_path is local to my computer.\n",
    "parameters = {\"data_path\": \"/home/diedre/Dropbox/bigdata/brats/2020/train\"}\n",
    "run = mlflow.projects.run('.', entry_point=\"pre_process\", use_conda=False, \n",
    "                          parameters=parameters,\n",
    "                          experiment_name=\"pre_processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell generates a visualization of the pre-processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020/05/25 20:36:55 INFO mlflow.projects: === Created directory /tmp/tmpcmax9qp4 for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2020/05/25 20:36:55 INFO mlflow.projects: === Running command 'python3 src/visualize_npz.py --input /home/diedre/Dropbox/bigdata/brats/2020/train/BraTS20_Training_001/BraTS20_Training_001_preprocessed.npz --no_display' in run with ID 'e9b970b1a09f49de946fbe6aed2565ae' === \n",
      "2020/05/25 20:36:57 INFO mlflow.projects: === Run (ID 'e9b970b1a09f49de946fbe6aed2565ae') succeeded ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# CODE CELL: 2\n",
    "# TODO use someway to distribute data. Currently input is local to my computer.\n",
    "parameters = {\"input\": \"/home/diedre/Dropbox/bigdata/brats/2020/train/BraTS20_Training_001/BraTS20_Training_001_preprocessed.npz\"}\n",
    "run = mlflow.projects.run('.', entry_point=\"visualize\", use_conda=False, \n",
    "                          parameters=parameters,\n",
    "                          experiment_name=\"pre_processing\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visualize](figures/display.png)\n",
    "Figure 2: Generated by code cell 2. The four modalities are showcased, in order: FLAIR, T1, T1 with Contrast and T2. Also displayed in the bottom row are the four segmentation targets: background, edema, non-enhancing tumor and enhancing tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method, named BTRSeg, leverages modified, fully 3D, attention based, UNet like encoder-decoder CNNs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments are being performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results show that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] AAAA<br>\n",
    "[2] BBBB<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems:<br>\n",
    "-How to use references in Jupyter?<br>\n",
    "-Image captions? Tables?<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
